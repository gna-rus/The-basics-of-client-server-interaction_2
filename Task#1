import requests
from bs4 import BeautifulSoup
import urllib.parse
from datetime import datetime, time, timedelta
import time
import re
import json

from fake_useragent import UserAgent
ua = UserAgent()



# Get a random browser user-agent string

fake_user = ua.random # создаю фейкового юзера

# Запрос веб-страницы
name_work = 'Data+science'
url = f'https://hh.ru/search/vacancy?text={name_work}&from=suggest_post&area=1&hhtmFrom=main&hhtmFromLabel=vacancy_search_line'
response = requests.get(url, headers = {'User-Agent' : fake_user})

# Парсинг HTML-содержимого веб-страницы с помощью Beautiful Soup
soup = BeautifulSoup(response.content, 'html.parser')

#
# Вывод ссылок на
release_links = []
for link in soup.find_all('div', ('class', 'magritte-card___bhGKz_6-1-14 magritte-card-border-radius-24___o72BE_6-1-14 magritte-card-stretched___0Uc0J_6-1-14 magritte-card-action___4A43B_6-1-14 magritte-card-shadow-on-hover___BoRL3_6-1-14 magritte-card-with-border___3KsrG_6-1-14')):
    try:
        release_links.append(link.find('a').get('href'))
    except:
        release_links.append('None')


#Извлечение данных из таблицы построково и сохранение их в списке словарей

def info_about_work(text, teg):
    table = text.find('div', {'class': teg})
    # class="magritte-h-spacing-container___rrYJZ_2-0-31"

    if not table:
        return 'error'
    else:
        return table.get_text("|||").split("|||")

def dict_about_all_work(tabel, workgiver):
    dict = {}
    tabel.remove(' ')
    print('до ' in tabel)
    if 'Уровень дохода не указан' in tabel:
        dict[tabel[0]] = [workgiver[-1], tabel[1], "".join(tabel[2:5])]
    elif 'до ' in tabel:
        salary = str(" ".join(tabel[1:6])).replace(r'\xa0000', 'тыс.')
        dict[tabel[0]] = [workgiver[-1], salary, "".join(tabel[6:9])]
    elif 'от ' in tabel:
        salary = str(" ".join(tabel[1:6])).replace(r'\xa0000', 'тыс.')
        dict[tabel[0]] = [workgiver[-1], salary, "".join(tabel[6:9])]
    return dict


data = {}
count = 1
for url in release_links:
    try:
        print('Шаг: ', count, ' URL:', url)

        response = requests.get(url, headers = {'User-Agent' : fake_user})
        soup = BeautifulSoup(response.content, 'html.parser')

        table = info_about_work(soup, 'magritte-card___bhGKz_6-1-14')
        print(table)


        work_giver = info_about_work(soup, 'magritte-h-spacing-container___rrYJZ_2-0-31')
        print(work_giver)

        if work_giver == 'error' or table == 'error':
            continue

        data_dict = {}


        print(dict_about_all_work(table, work_giver))


        count += 1
        if count == 15:
            break

        time.sleep(5)

    except:
        pass




    #     row_data={}
    #     for row in rows:
    #         key = row.find('span').text.strip()
    #         value = row.find_all('span')[1].text.strip()
    #         if key == 'Opening':
    #             value = int(re.sub('[^0-9]', '', value))
    #         elif key == 'Release Date':
    #             value = value
    #         elif key == 'Running Time':
    #             time_delta = datetime.strptime(value, '%H hr %M min') - datetime(1900, 1, 1)
    #             value = time_delta.total_seconds()
    #         elif key == 'Genres':
    #             value = [genre.strip() for genre in value.split('\n') if genre.strip()]
    #         elif key == 'In Release':
    #             value = value.replace(' days/3 weeks', '').strip()
    #         elif key == 'Widest Release':
    #             value = int(re.sub('[^0-9]', '', value))
    #
    #         row_data[key] = value
    #

# print(data)
# # сохранение данных в JSON-файл
# with open('box_office_data.json', 'w') as f:
#     json.dump(data, f)
