import requests
from bs4 import BeautifulSoup
import urllib.parse
from datetime import datetime, time, timedelta
import time
import re
import json

from fake_useragent import UserAgent
ua = UserAgent()



# Get a random browser user-agent string
fake_user = ua.random # создаю фейкового юзера

# Запрос веб-страницы
name_work = 'Data+science' # Провожу поиск на сайте по этому тексту
url = f'https://hh.ru/search/vacancy?text={name_work}&from=suggest_post&area=1&hhtmFrom=main&hhtmFromLabel=vacancy_search_line'
response = requests.get(url, headers = {'User-Agent' : fake_user}) # Получаю get запрос по API

# Парсинг HTML-содержимого веб-страницы с помощью Beautiful Soup
soup = BeautifulSoup(response.content, 'html.parser')

#
# Создаю списко ссылок вакансий
release_links = []
for link in soup.find_all('div', ('class', 'magritte-card___bhGKz_6-1-14 magritte-card-border-radius-24___o72BE_6-1-14 magritte-card-stretched___0Uc0J_6-1-14 magritte-card-action___4A43B_6-1-14 magritte-card-shadow-on-hover___BoRL3_6-1-14 magritte-card-with-border___3KsrG_6-1-14')):
    try:
        release_links.append(link.find('a').get('href'))
    except:
        release_links.append('None')




def info_about_work(text, teg):
    """Функция извлечения информации со страницы о вакансии
    На вход подается результат парсинга HTML-содержимого веб-страницы с помощью Beautiful Soup и
    class по которому будет проводиться поиск содержимого.
    На выход передается или общая информация о вакансии или название кампании"""
    table = text.find('div', {'class': teg})
    if not table:
        return 'error'
    else:
        return table.get_text("|||").split("|||")

def dict_about_all_work(tabel, workgiver):
    """Функция форматирования и обработки данных о вакансии
    На вход подается результат обработки страницы и
    и название компании
    """
    dict = {}
    tabel.remove(' ')

    if 'Уровень дохода не указан' in tabel:
        dict[tabel[0]] = [workgiver[-1], tabel[1], "".join(tabel[2:5])]
    elif 'до ' in tabel:
        salary = str(" ".join(tabel[1:6])).replace(r'\xa0000', 'тыс.')
        dict[tabel[0]] = [workgiver[-1], salary, "".join(tabel[6:9])]
    elif 'от ' in tabel:
        salary = str(" ".join(tabel[1:6])).replace(r'\xa0000', 'тыс.')
        dict[tabel[0]] = [workgiver[-1], salary, "".join(tabel[6:9])]
    return dict


data = {}
lim = 5 # задаю количестко вакансий которое необходимо обработать
count = 1
for url in release_links:
    try:
        print('Шаг: ', count, ' URL:', url) # 

        response = requests.get(url, headers = {'User-Agent' : fake_user}) # провожу get запрос по API для отдельной взятой вакансии
        soup = BeautifulSoup(response.content, 'html.parser') # Парсинг HTML-содержимого веб-страницы с помощью Beautiful Soup
        table = info_about_work(soup, 'magritte-card___bhGKz_6-1-14')

        work_giver = info_about_work(soup, 'magritte-h-spacing-container___rrYJZ_2-0-31')

        # Проверка, если не получилось распирсить страницу то цикл прерывается
        if work_giver == 'error' or table == 'error':
            continue

        data_dict = {}

        data_dict = dict_about_all_work(table, work_giver) # Получаем словарь где ключ - это название вакансии,
                                                           # а значение содержит список с названием капании, требованием по опыты и з\п
        print(data_dict)

        count += 1
        if count == lim:
            break

        time.sleep(5)
        data.update(data_dict) # Создаю большой списко в который записываются все результаты обработки вакансий

    except:
        pass

print('Результат: \n',data)
# сохранение данных в JSON-файл
with open('data.json', 'w', encoding="utf-8") as f:
    json.dump(data, f)
