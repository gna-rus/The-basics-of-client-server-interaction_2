import requests
from bs4 import BeautifulSoup
import urllib.parse
from datetime import datetime, time, timedelta
import time
import re
import json

from fake_useragent import UserAgent
ua = UserAgent()



# Get a random browser user-agent string

fake_user = ua.random # создаю фейкового юзера

# Запрос веб-страницы
name_work = 'Data+science'
url = f'https://hh.ru/search/vacancy?text={name_work}&from=suggest_post&area=1&hhtmFrom=main&hhtmFromLabel=vacancy_search_line'
response = requests.get(url, headers = {'User-Agent' : fake_user})

# Парсинг HTML-содержимого веб-страницы с помощью Beautiful Soup
soup = BeautifulSoup(response.content, 'html.parser')

#
# Вывод ссылок на
release_links = []
for link in soup.find_all('div', ('class', 'magritte-card___bhGKz_6-1-14 magritte-card-border-radius-24___o72BE_6-1-14 magritte-card-stretched___0Uc0J_6-1-14 magritte-card-action___4A43B_6-1-14 magritte-card-shadow-on-hover___BoRL3_6-1-14 magritte-card-with-border___3KsrG_6-1-14')):
    try:
        release_links.append(link.find('a').get('href'))
    except:
        release_links.append('None')


#Извлечение данных из таблицы построково и сохранение их в списке словарей

def info_about_work(text, teg):
    table = text.find('div', {'class': teg})
    # class="magritte-h-spacing-container___rrYJZ_2-0-31"

    if not table:
        return 'error'
    else:
        return table.get_text("|||").split("|||")

def dict_about_all_work(tabel, workgiver):
    dict = {}
    tabel.remove(' ')

    if 'Уровень дохода не указан' in tabel:
        dict[tabel[0]] = [workgiver[-1], tabel[1], "".join(tabel[2:5])]
    elif 'до ' in tabel:
        salary = str(" ".join(tabel[1:6])).replace(r'\xa0000', 'тыс.')
        dict[tabel[0]] = [workgiver[-1], salary, "".join(tabel[6:9])]
    elif 'от ' in tabel:
        salary = str(" ".join(tabel[1:6])).replace(r'\xa0000', 'тыс.')
        dict[tabel[0]] = [workgiver[-1], salary, "".join(tabel[6:9])]
    return dict


data = {}
count = 1
for url in release_links:
    try:
        print('Шаг: ', count, ' URL:', url)

        response = requests.get(url, headers = {'User-Agent' : fake_user})
        soup = BeautifulSoup(response.content, 'html.parser')
        table = info_about_work(soup, 'magritte-card___bhGKz_6-1-14')


        work_giver = info_about_work(soup, 'magritte-h-spacing-container___rrYJZ_2-0-31')

        if work_giver == 'error' or table == 'error':
            continue

        data_dict = {}


        data_dict = dict_about_all_work(table, work_giver)
        print(data_dict)

        count += 1
        if count == 5:
            break

        time.sleep(5)
        data.update(data_dict)

    except:
        pass

print('Результат: \n',data)
# сохранение данных в JSON-файл
with open('data.json', 'w', encoding="utf-8") as f:
    json.dump(data, f)
